{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T07:40:15.618474Z",
     "start_time": "2019-06-26T07:40:13.078027Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using plaidml.keras.backend backend.\n",
      "INFO:plaidml:Opening device \"metal_amd_radeon_pro_555x.0\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               6422784   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 21,140,042\n",
      "Trainable params: 6,425,354\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import os  # a conflict in my mac system\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from keras.applications.nasnet import NASNetMobile\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "# from classification_models.resnet import ResNet18, preprocess_input\n",
    "\n",
    "from keras.applications import VGG16\n",
    "conv_base = VGG16(\n",
    "    weights='imagenet',\n",
    "    include_top=\n",
    "    False,  #we are going to remove the top layer, VGG was trained for 1000 classes, here we only have two\n",
    "    input_shape=(224, 224, 3))\n",
    "\n",
    "# img size 224*224\n",
    "# https://keras.io/applications/#mobilenetv2\n",
    "# mobile_base = NASNetMobile(input_shape=(224,224,3), include_top=False, weights='imagenet', input_tensor=None, pooling=None, classes=1000)\n",
    "# mobile_base = ResNet18((224, 224, 3), include_top=False, weights='imagenet')\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "# this \"freezes\"\n",
    "conv_base.trainable = False\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fine Tuning\n",
    "# # we freeze all layers before block5_conv1\n",
    "# conv_base.trainable = True\n",
    "# set_trainable = False\n",
    "# for layer in conv_base.layers:\n",
    "#     if layer.name == 'block5_conv1':\n",
    "#         set_trainable = True\n",
    "#     if set_trainable:\n",
    "#         layer.trainable = True\n",
    "#     else:\n",
    "#         layer.trainable = False\n",
    "#\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T06:35:29.323384Z",
     "start_time": "2019-06-26T06:35:28.522123Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate val set\n",
    "import os, shutil\n",
    "\n",
    "train_dir = \"/Users/ranxin/Downloads/state-farm-distracted-driver-detection/imgs/train\"\n",
    "val_dir = \"/Users/ranxin/Downloads/state-farm-distracted-driver-detection/imgs/val\"\n",
    "\n",
    "train_c0_dir=os.path.join(train_dir, 'c0')\n",
    "train_c1_dir=os.path.join(train_dir, 'c1')\n",
    "train_c2_dir=os.path.join(train_dir, 'c2')\n",
    "train_c3_dir=os.path.join(train_dir, 'c3')\n",
    "train_c4_dir=os.path.join(train_dir, 'c4')\n",
    "train_c5_dir=os.path.join(train_dir, 'c5')\n",
    "train_c6_dir=os.path.join(train_dir, 'c6')\n",
    "train_c7_dir=os.path.join(train_dir, 'c7')\n",
    "train_c8_dir=os.path.join(train_dir, 'c8')\n",
    "train_c9_dir=os.path.join(train_dir, 'c9')\n",
    "\n",
    "val_c0_dir=os.path.join(val_dir, 'c0')\n",
    "val_c1_dir=os.path.join(val_dir, 'c1')\n",
    "val_c2_dir=os.path.join(val_dir, 'c2')\n",
    "val_c3_dir=os.path.join(val_dir, 'c3')\n",
    "val_c4_dir=os.path.join(val_dir, 'c4')\n",
    "val_c5_dir=os.path.join(val_dir, 'c5')\n",
    "val_c6_dir=os.path.join(val_dir, 'c6')\n",
    "val_c7_dir=os.path.join(val_dir, 'c7')\n",
    "val_c8_dir=os.path.join(val_dir, 'c8')\n",
    "val_c9_dir=os.path.join(val_dir, 'c9')\n",
    "\n",
    "n=0\n",
    "files= os.listdir(train_c0_dir)\n",
    "for i in files:\n",
    "    if not os.path.isdir(i):\n",
    "        src = os.path.join(train_c0_dir, i)\n",
    "        dst = os.path.join(val_c0_dir, i)\n",
    "        shutil.move(src, dst)\n",
    "        n=n+1\n",
    "        if n>400:\n",
    "            break\n",
    "            \n",
    "            \n",
    "n=0\n",
    "files= os.listdir(train_c1_dir)\n",
    "for i in files:\n",
    "    if not os.path.isdir(i):\n",
    "        src = os.path.join(train_c1_dir, i)\n",
    "        dst = os.path.join(val_c1_dir, i)\n",
    "        shutil.move(src, dst)\n",
    "        n=n+1\n",
    "        if n>400:\n",
    "            break\n",
    "            \n",
    "            \n",
    "n=0\n",
    "files= os.listdir(train_c2_dir)\n",
    "for i in files:\n",
    "    if not os.path.isdir(i):\n",
    "        src = os.path.join(train_c2_dir, i)\n",
    "        dst = os.path.join(val_c2_dir, i)\n",
    "        shutil.move(src, dst)\n",
    "        n=n+1\n",
    "        if n>400:\n",
    "            break\n",
    "            \n",
    "            \n",
    "n=0\n",
    "files= os.listdir(train_c3_dir)\n",
    "for i in files:\n",
    "    if not os.path.isdir(i):\n",
    "        src = os.path.join(train_c3_dir, i)\n",
    "        dst = os.path.join(val_c3_dir, i)\n",
    "        shutil.move(src, dst)\n",
    "        n=n+1\n",
    "        if n>400:\n",
    "            break\n",
    "            \n",
    "            \n",
    "n=0\n",
    "files= os.listdir(train_c4_dir)\n",
    "for i in files:\n",
    "    if not os.path.isdir(i):\n",
    "        src = os.path.join(train_c4_dir, i)\n",
    "        dst = os.path.join(val_c4_dir, i)\n",
    "        shutil.move(src, dst)\n",
    "        n=n+1\n",
    "        if n>400:\n",
    "            break\n",
    "            \n",
    "            \n",
    "n=0\n",
    "files= os.listdir(train_c5_dir)\n",
    "for i in files:\n",
    "    if not os.path.isdir(i):\n",
    "        src = os.path.join(train_c5_dir, i)\n",
    "        dst = os.path.join(val_c5_dir, i)\n",
    "        shutil.move(src, dst)\n",
    "        n=n+1\n",
    "        if n>400:\n",
    "            break\n",
    "            \n",
    "            \n",
    "n=0\n",
    "files= os.listdir(train_c6_dir)\n",
    "for i in files:\n",
    "    if not os.path.isdir(i):\n",
    "        src = os.path.join(train_c6_dir, i)\n",
    "        dst = os.path.join(val_c6_dir, i)\n",
    "        shutil.move(src, dst)\n",
    "        n=n+1\n",
    "        if n>400:\n",
    "            break\n",
    "            \n",
    "            \n",
    "n=0\n",
    "files= os.listdir(train_c7_dir)\n",
    "for i in files:\n",
    "    if not os.path.isdir(i):\n",
    "        src = os.path.join(train_c7_dir, i)\n",
    "        dst = os.path.join(val_c7_dir, i)\n",
    "        shutil.move(src, dst)\n",
    "        n=n+1\n",
    "        if n>400:\n",
    "            break\n",
    "            \n",
    "            \n",
    "n=0\n",
    "files= os.listdir(train_c8_dir)\n",
    "for i in files:\n",
    "    if not os.path.isdir(i):\n",
    "        src = os.path.join(train_c8_dir, i)\n",
    "        dst = os.path.join(val_c8_dir, i)\n",
    "        shutil.move(src, dst)\n",
    "        n=n+1\n",
    "        if n>400:\n",
    "            break\n",
    "            \n",
    "            \n",
    "n=0\n",
    "files= os.listdir(train_c9_dir)\n",
    "for i in files:\n",
    "    if not os.path.isdir(i):\n",
    "        src = os.path.join(train_c9_dir, i)\n",
    "        dst = os.path.join(val_c9_dir, i)\n",
    "        shutil.move(src, dst)\n",
    "        n=n+1\n",
    "        if n>400:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T07:40:23.391368Z",
     "start_time": "2019-06-26T07:40:22.823780Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18414 images belonging to 10 classes.\n",
      "Found 4010 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dir = \"/Users/ranxin/Downloads/state-farm-distracted-driver-detection/imgs/train\"\n",
    "test_dir = \"/Users/ranxin/Downloads/state-farm-distracted-driver-detection/imgs/val\"\n",
    "\n",
    "train_data_generator = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "test_data_generator = ImageDataGenerator(rescale=1. / 255)\n",
    "train_generator = train_data_generator.flow_from_directory(\n",
    "    train_dir, target_size=(224, 224), batch_size=40, class_mode='categorical')\n",
    "test_generator = test_data_generator.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=40,\n",
    "    class_mode='categorical')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T07:40:25.230034Z",
     "start_time": "2019-06-26T07:40:24.662979Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "    metrics=['acc'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-06-26T07:46:00.855Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Analyzing Ops: 71 of 251 operations complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/400 [======>.......................] - ETA: 10:41 - loss: 0.9669 - acc: 0.6951"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Analyzing Ops: 62 of 251 operations complete\n",
      "INFO:plaidml:Analyzing Ops: 120 of 251 operations complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399/400 [============================>.] - ETA: 2s - loss: 0.9712 - acc: 0.6930"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Analyzing Ops: 73 of 179 operations complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 1027s 3s/step - loss: 0.9712 - acc: 0.6931 - val_loss: 0.4416 - val_acc: 0.8747\n",
      "Epoch 2/2\n",
      "399/400 [============================>.] - ETA: 2s - loss: 0.9503 - acc: 0.7005"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Analyzing Ops: 61 of 179 operations complete\n",
      "INFO:plaidml:Analyzing Ops: 112 of 179 operations complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "400/400 [==============================] - 1020s 3s/step - loss: 0.9508 - acc: 0.7002 - val_loss: 0.4441 - val_acc: 0.8691\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('Distracted_vgg16.h5', by_name=True)\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=400,\n",
    "    epochs=2,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=80)\n",
    "\n",
    "model.save_weights('Distracted_vgg16.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T07:45:04.246127Z",
     "start_time": "2019-06-26T07:45:03.037057Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save_weights('Distracted_vgg16.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict class of new input\n",
    "img_path = \"/Users/ranxin/Downloads/state-farm-distracted-driver-detection/imgs/test/img_1.jpg\"\n",
    "import cv2\n",
    "new_input = cv2.imread(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_input = cv2.resize(new_input, (224, 224), interpolation=cv2.INTER_CUBIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('input', new_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('Distracted_vgg16.h5', by_name=True)\n",
    "model.save('Distracted_vgg16_full.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('Distracted_vgg16_full.h5')\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640, 3)\n",
      "(224, 224, 3)\n",
      "<class 'numpy.ndarray'> [5]\n"
     ]
    }
   ],
   "source": [
    "img_path = \"/Users/ranxin/Downloads/state-farm-distracted-driver-detection/imgs/test/img_1.jpg\"\n",
    "import cv2\n",
    "\n",
    "new_input = cv2.imread(img_path)\n",
    "print(new_input.shape)\n",
    "new_input = cv2.resize(new_input, (224, 224), interpolation=cv2.INTER_CUBIC)\n",
    "print(new_input.shape)\n",
    "# cv2.imwrite('new_input.jpg', new_input)\n",
    "new_input = new_input.reshape((-1, 224, 224, 3))\n",
    "ynew = model.predict_classes(new_input)\n",
    "print(type(ynew), ynew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               6422784   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 21,140,042\n",
      "Trainable params: 21,140,042\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot create group in read only mode.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0c7700de67a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/ranxin/Downloads/state-farm-distracted-driver-detection/imgs/models/model_ex-001_acc-0.355362.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deserialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mopened_new_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36m_deserialize_model\u001b[0;34m(f, custom_objects, compile)\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m     \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No model found in config.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/utils/io_utils.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot create group in read only mode.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mH5Dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot create group in read only mode."
     ]
    }
   ],
   "source": [
    "import os  # a conflict in my mac system\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import load_model\n",
    "model = load_model(\"/Users/ranxin/Downloads/state-farm-distracted-driver-detection/imgs/models/model_ex-001_acc-0.355362.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are trying to load a weight file containing 242 layers into a model with 0 layers.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-104b26d77a18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model.load_weights(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;34m\"/Users/ranxin/Downloads/state-farm-distracted-driver-detection/imgs/models/model_ex-001_acc-0.355362.h5\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m )\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   1164\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m                 saving.load_weights_from_hdf5_group(\n\u001b[0;32m-> 1166\u001b[0;31m                     f, self.layers, reshape=reshape)\n\u001b[0m\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers, reshape)\u001b[0m\n\u001b[1;32m   1028\u001b[0m                          \u001b[0;34m'containing '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m                          \u001b[0;34m' layers into a model with '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1030\u001b[0;31m                          str(len(filtered_layers)) + ' layers.')\n\u001b[0m\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m     \u001b[0;31m# We batch weight value assignments in a single backend call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You are trying to load a weight file containing 242 layers into a model with 0 layers."
     ]
    }
   ],
   "source": [
    "\n",
    "model.load_weights(\n",
    "    \"/Users/ranxin/Downloads/state-farm-distracted-driver-detection/imgs/models/model_ex-001_acc-0.355362.h5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
